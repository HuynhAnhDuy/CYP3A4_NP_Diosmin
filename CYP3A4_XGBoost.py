import numpy as np
import pandas as pd
from datetime import datetime
import os

# ==== Ch·ªâ c·∫ßn ch·ªânh 1 d√≤ng n√†y n·∫øu ƒë·ªïi prefix ====
BASE_PREFIX = "Hepatotoxicity"

# XGBoost
try:
    from xgboost import XGBClassifier
except ImportError as e:
    raise SystemExit("XGBoost ch∆∞a ƒë∆∞·ª£c c√†i. C√†i b·∫±ng: pip install xgboost") from e

from sklearn.metrics import (
    accuracy_score, recall_score, confusion_matrix, matthews_corrcoef,
    roc_curve, auc, precision_score, f1_score, precision_recall_curve,
    balanced_accuracy_score,
)

# === Hu·∫•n luy·ªán 1 model XGBoost tr√™n 1 feature set ===
def train_xgboost(
    x_train, x_test, y_train, y_test,
    n_estimators=500, max_depth=6, random_state=42,
    n_jobs=-1, learning_rate=0.05, subsample=0.8, colsample_bytree=0.8,
    reg_alpha=0.1, reg_lambda=1.0, gamma=0.1, min_child_weight=1
):
    x_train = np.asarray(x_train)
    x_test  = np.asarray(x_test)
    y_train = np.asarray(y_train).ravel()
    y_test  = np.asarray(y_test).ravel()

    # T√≠nh scale_pos_weight n·∫øu d·ªØ li·ªáu l·ªách l·ªõp
    n_pos = np.sum(y_train == 1)
    n_neg = np.sum(y_train == 0)
    scale_pos_weight = float(n_neg) / float(n_pos) if n_pos > 0 else 1.0

    params = dict(
        objective="binary:logistic",
        n_estimators=n_estimators,
        max_depth=max_depth,
        learning_rate=learning_rate,
        subsample=subsample,
        colsample_bytree=colsample_bytree,
        reg_alpha=reg_alpha,
        reg_lambda=reg_lambda,
        gamma=gamma,
        min_child_weight=min_child_weight,
        random_state=random_state,
        n_jobs=n_jobs,
        tree_method="hist",  # ƒë·ªïi 'gpu_hist' n·∫øu d√πng GPU
        eval_metric="logloss",
        scale_pos_weight=scale_pos_weight,
        use_label_encoder=False
    )

    clf = XGBClassifier(**params)
    clf.fit(
        x_train, y_train,
        eval_set=[(x_test, y_test)],
        verbose=True
    )

    # D·ª± ƒëo√°n
    y_pred      = clf.predict(x_test)
    y_prob_test = clf.predict_proba(x_test)[:, 1]
    y_prob_train = clf.predict_proba(x_train)[:, 1]

    # Metrics tr√™n TEST
    accuracy     = accuracy_score(y_test, y_pred)
    balanced_acc = balanced_accuracy_score(y_test, y_pred)
    mcc          = matthews_corrcoef(y_test, y_pred)
    precision    = precision_score(y_test, y_pred, zero_division=0)
    recall       = recall_score(y_test, y_pred, zero_division=0)
    f1           = f1_score(y_test, y_pred, zero_division=0)

    labels = np.unique(y_test)
    if set(labels) == {0, 1}:
        tn, fp, fn, tp = confusion_matrix(y_test, y_pred, labels=[0, 1]).ravel()
        specificity = tn / (tn + fp) if (tn + fp) > 0 else np.nan
    else:
        specificity = np.nan

    fpr, tpr, _ = roc_curve(y_test, y_prob_test)
    roc_auc = auc(fpr, tpr)

    prec_arr, rec_arr, _ = precision_recall_curve(y_test, y_prob_test)
    pr_auc = auc(rec_arr, prec_arr)

    return {
        "metrics": {
            "Accuracy": accuracy,
            "Balanced Accuracy": balanced_acc,
            "AUROC": roc_auc,
            "AUPRC": pr_auc,
            "MCC": mcc,
            "Precision": precision,
            "Sensitivity": recall,
            "Specificity": specificity,
            "F1": f1
        },
        "y_prob_train": y_prob_train,
        "y_prob_test": y_prob_test,
        "y_train_true": y_train,
        "y_test_true": y_test
    }

# === Ch·∫°y XGB qua t·∫•t c·∫£ feature sets ===
def run_all_feature_sets(feature_sets, num_runs=3):
    results_all = {}
    all_metrics_raw = []

    # === T·∫°o th∆∞ m·ª•c ch·ª©a y_prob theo timestamp ===
    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    prob_folder = f"Prob_Hepa_XGB/Prob_{timestamp}"
    os.makedirs(prob_folder, exist_ok=True)
    print(f"\nüìÅ S·∫Ω l∆∞u y_prob v√†o: {prob_folder}")

    for fs in feature_sets:
        print(f"\n=== Evaluating feature set: {fs.upper()} ===")
        fs_file = fs.lower()

        try:
            # L∆∞u √Ω: index_col=0 v√¨ c·ªôt ƒë·∫ßu l√† Index
            x_train = pd.read_csv(f"{BASE_PREFIX}_x_train_{fs_file}.csv", index_col=0).values
            x_test  = pd.read_csv(f"{BASE_PREFIX}_x_test_{fs_file}.csv", index_col=0).values

            y_train = pd.read_csv(f"{BASE_PREFIX}_y_train.csv", index_col=0).values.ravel()
            y_test  = pd.read_csv(f"{BASE_PREFIX}_y_test.csv", index_col=0).values.ravel()
        except FileNotFoundError as e:
            print(f"[SKIP] Thi·∫øu file cho {fs.upper()}: {e}")
            continue

        metrics_keys = [
            "Accuracy", "Balanced Accuracy", "AUROC", "AUPRC",
            "MCC", "Precision", "Sensitivity", "Specificity", "F1"
        ]
        metrics_summary = {k: [] for k in metrics_keys}

        for run in range(num_runs):
            seed = 42 + run
            print(f"\nüöÄ Run {run+1}/{num_runs} for {fs.upper()} (seed={seed})...")
            result = train_xgboost(
                x_train, x_test, y_train, y_test,
                n_estimators=500, max_depth=6, random_state=seed,
                n_jobs=-1
            )

            metrics = result["metrics"]
            for k in metrics_keys:
                metrics_summary[k].append(metrics[k])

            metrics["Feature_Set"] = fs.upper()
            metrics["Run"] = run + 1
            metrics["Seed"] = seed
            all_metrics_raw.append(metrics)

            # === L∆∞u y_prob train/test ===
            train_df = pd.DataFrame({
                "y_true": result["y_train_true"],
                "y_prob": result["y_prob_train"]
            })
            test_df = pd.DataFrame({
                "y_true": result["y_test_true"],
                "y_prob": result["y_prob_test"]
            })

            train_path = f"{prob_folder}/{BASE_PREFIX}_train_prob_{fs_file}_run{run+1}.csv"
            test_path  = f"{prob_folder}/{BASE_PREFIX}_test_prob_{fs_file}_run{run+1}.csv"

            train_df.to_csv(train_path, index=False)
            test_df.to_csv(test_path, index=False)

            print(f"üíæ ƒê√£ l∆∞u: {train_path}, {test_path}")

        # Mean ¬± SD cho t·ª´ng feature set
        summary = {k: (np.nanmean(v), np.nanstd(v)) for k, v in metrics_summary.items()}
        results_all[fs] = summary

        print(f"\nüìä --- {fs.upper()} Results (Mean ¬± SD over {num_runs} runs) ---")
        for k, (mean_val, std_val) in summary.items():
            print(f"{k}: {mean_val:.3f} ¬± {std_val:.3f}")

    # Xu·∫•t file raw t·ª´ng run
    df_raw = pd.DataFrame(all_metrics_raw)
    df_raw.to_csv(f"{BASE_PREFIX}_XGB_feature_sets_metrics_raw.csv", index=False)
    print(f"\n‚úÖ Saved raw results: {BASE_PREFIX}_XGB_feature_sets_metrics_raw.csv")

    return results_all

# === H√†m ch√≠nh ===
def main():
    # 8 b·ªô ƒë·∫∑c tr∆∞ng t∆∞∆°ng ·ª©ng v·ªõi 8 file X b·∫°n ƒëang c√≥
    feature_sets = [
        "ecfp",
        "rdkit",
        "maccs",
        "phychem",
        "estate",
        "substruct",         # Hepatotoxicity_x_train_substruct.csv
        "all_features",      # Hepatotoxicity_x_train_all_features.csv
        "selected_features"  # Hepatotoxicity_x_train_selected_features.csv
    ]

    results_by_fs = run_all_feature_sets(feature_sets, num_runs=3)

    # Xu·∫•t b·∫£ng Mean ¬± SD
    df_export = pd.DataFrame({
        fs.upper(): {metric: f"{mean:.3f} ¬± {std:.3f}" for metric, (mean, std) in metrics.items()}
        for fs, metrics in results_by_fs.items()
    }).T

    df_export.to_csv(f"{BASE_PREFIX}_XGB_feature_sets_metrics.csv")
    print(f"\n‚úÖ Saved summary: {BASE_PREFIX}_XGB_feature_sets_metrics.csv")


if __name__ == "__main__":
    main()
